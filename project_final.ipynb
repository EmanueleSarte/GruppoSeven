{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> RESPIRATION RATE ESTIMATION\n",
    " \n",
    " **08/02/2023**   \n",
    "    \n",
    " **Group 7:** **Giovanni Zago**, **Enrico Lupi**, **Emanuele Sarte**, **Alessio Saccomani** \n",
    " \n",
    "   The aim of this project is to estimate the Respiratory Rate (RR) by Seismocardiography(SCG), a technique where the detector is positioned above sternum, and Ballistocardiography (BCG), in which there is no contact between the sensor and the body: in this study the sensor was placed....). The measurements were taken using the detector MuSe (Multi-sensor miniaturized, low-power, wireless Inertial Measurement Unit), provided by 221e (https://www.221e.com). An IMU is a combination of an accelerometer and a gyroscope sensor, capable of detecting movements and measuring their intensity in terms of acceleration and rotational speeds. Sometimes, like in this case, a magnetometer is also included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.221e.com/wp-content/uploads/2022/10/221e-Muse1.jpg\" width=\"400\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "Image(url= \"https://www.221e.com/wp-content/uploads/2022/10/221e-Muse1.jpg\", width=400, height=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://embeddedinventor.com/wp-content/uploads/2019/07/imu1.jpg?ezimgfmt=ng:webp/ngcb7\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://embeddedinventor.com/wp-content/uploads/2019/07/imu1.jpg?ezimgfmt=ng:webp/ngcb7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and first steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we imported the necessary libraries for the analysis (pandas, numpy, matplotlib, scipy, ecc.). \n",
    "\n",
    "We then downloaded the datafile 'center_sternum.txt' as a pandas dataframe. For the following analysis we decided to take into account the linear acceleration (in mg), the angular velocity (in degrees per second) and the magnetic field (in mgauss) in all three directions; the quaternions were not considered, instead, so they were dropped. Afterwards, we calibrated the measurements using the information in the file README_1.txt and we added to the dataset the absolute time at which every measuremnt is taken: as the data collection frequency is 200 Hz, each measurement is taken every 5 ms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import fftpack\n",
    "from scipy import fft\n",
    "from scipy import signal\n",
    "from scipy import optimize\n",
    "from scipy import linalg\n",
    "import seaborn as sns\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"center_sternum.txt\"\n",
    "raw_data = pd.read_csv(file_name, sep=\"\\t\")\n",
    "\n",
    "# We drop the columns we're not going to use\n",
    "raw_data = raw_data.drop([\"Log Mode\", \"qw\", \"qi\", \"qj\", \"qk\"], axis=1)\n",
    "\n",
    "# We look that the frequence is the same in all the dataset\n",
    "if len(raw_data[\"Log Freq\"].unique()) != 1:\n",
    "    print(\"There is more than a frequency\")\n",
    "    exit(1)\n",
    "\n",
    "# We select the frequency (200 Hz)\n",
    "ACQ_FREQ = raw_data.loc[0, \"Log Freq\"] #type: int\n",
    "\n",
    "# We create a column with the absolute time by multiplying 1/200 s by integers\n",
    "raw_data.insert(2, \"Abs Time\", np.arange(0, len(raw_data)) * (1 / ACQ_FREQ), allow_duplicates=False)\n",
    "\n",
    "raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_gyro = np.array([-2.242224, 2.963463, -0.718397])\n",
    "calibration_acc = np.array([[1.000966, -0.002326418, -0.0006995499],\n",
    "                            [-0.002326379, 0.9787045, -0.001540918],\n",
    "                            [-0.0006995811, -0.001540928, 1.00403]])\n",
    "offset_acc = np.array([-3.929942, -13.74679, 60.67546])\n",
    "calibration_magn = np.array([[0.9192851, -0.02325168, 0.003480837],\n",
    "                             [-0.02325175, 0.914876, 0.004257396],\n",
    "                             [0.003481006, 0.004257583, 0.8748001]])\n",
    "offset_magn = np.array([-95.67974, -244.9142, 17.71132])\n",
    "\n",
    "raw_data[['AccX', 'AccY', 'AccZ']] = np.dot(raw_data[['AccX', 'AccY', 'AccZ']], calibration_acc.T) + offset_acc.T\n",
    "raw_data[['GyroX', 'GyroY', 'GyroZ']] = raw_data[['GyroX', 'GyroY', 'GyroZ']] + offset_gyro.T\n",
    "raw_data[['MagnX', 'MagnY', 'MagnZ']] = np.dot(raw_data[['MagnX', 'MagnY', 'MagnZ']], calibration_magn.T) + offset_magn.T\n",
    "raw_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "\n",
    "Now, we plot the accelerations, the angular velocity and the magnetic field with respect to the time. In this way we have a first idea about the data, in particular we understand which windows of time is ideal to studying the dataset. We look for times where there's not anomalous variation of acceleration or velocity (maybe some movements of the body independent from the hearth rate, and respiration rate) that we do not want to consider. In the left column there is the entire measure and in the right column there is a zoom to see a smaller window of time.\n",
    "We see that from 10 seconds to 65 seconds all the measures have a constant behaviour, so we consider this to be the time window we're going to use in the rest of the project. In the plot the window of times is highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # labels = [\"GyroX\", \"GyroY\", \"GyroZ\", \"AccX\", \"AccY\", \"AccZ\"]\n",
    "# Nvar = len(labels)\n",
    "\n",
    "# fig, axs = plt.subplots(Nvar, 2, figsize=(18, 4 * Nvar))\n",
    "# for i in range(2):\n",
    "#     for j in range(Nvar):\n",
    "\n",
    "#         if i == 0:\n",
    "#             x_range = raw_data[\"Abs Time\"]\n",
    "#             data_range = raw_data[labels[j]]\n",
    "#         else:\n",
    "#             ixmin, ixmax = (20 * ACQ_FREQ - 50,  24 * ACQ_FREQ + 50)\n",
    "#             data_range = raw_data.loc[ixmin:ixmax ,labels[j]]\n",
    "#             x_range = raw_data.loc[ixmin:ixmax, \"Abs Time\"]\n",
    "#             axs[j][i].set_xlim(20, 24)\n",
    "\n",
    "#         axs[j][i].plot(x_range, data_range, label=labels[j])\n",
    "#         axs[j][i].set_xlabel(\"Time [s]\")\n",
    "#         axs[j][i].set_ylabel(\"Magnitude\")\n",
    "#         axs[j][i].legend(loc=\"best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n",
    "\n",
    "labels = [\"GyroX\", \"GyroY\", \"GyroZ\", \"AccX\", \"AccY\", \"AccZ\", \"MagnX\", \"MagnY\", \"MagnZ\"]\n",
    "Nvar = len(labels)\n",
    "\n",
    "T1_CUT = 10     \n",
    "IT1_CUT = round(T1_CUT * ACQ_FREQ)\n",
    "T2_CUT = 65 # included\n",
    "IT2_CUT = round(T2_CUT * ACQ_FREQ) + 1\n",
    "\n",
    "plt.figure(figsize=(18, 4 * (Nvar // 3)))\n",
    "for i in range(Nvar // 3):\n",
    "    ax = plt.subplot(Nvar // 3, 1, i + 1)\n",
    "    plt.plot(raw_data[\"Abs Time\"], raw_data[labels[i * 3]], label=labels[i * 3])\n",
    "    plt.plot(raw_data[\"Abs Time\"], raw_data[labels[i * 3 + 1]], label=labels[i * 3 + 1])\n",
    "    plt.plot(raw_data[\"Abs Time\"], raw_data[labels[i * 3 + 2]], label=labels[i * 3 + 2])\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.vlines(x=raw_data.loc[IT1_CUT, \"Abs Time\"], ymin=ymin, ymax=ymax, colors=\"black\")\n",
    "    plt.vlines(x=raw_data.loc[IT2_CUT, \"Abs Time\"], ymin=ymin, ymax=ymax, colors=\"black\")\n",
    "\n",
    "    ax.fill_between(raw_data.loc[IT1_CUT: IT2_CUT, \"Abs Time\"], ymin, ymax, color='C0', alpha=0.25)\n",
    "\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "raw_data_cut = raw_data.loc[IT1_CUT: IT2_CUT - 1, :]\n",
    "raw_data_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(Nvar, 2, figsize=(18, 4 * Nvar))\n",
    "for i in range(2):\n",
    "    for j in range(Nvar):\n",
    "        if i == 0:\n",
    "            x_range = raw_data[\"Abs Time\"]\n",
    "            data_range = raw_data[labels[j]]\n",
    "        else:\n",
    "            xmax, xmin = 16, 24\n",
    "            ixmin, ixmax = (xmax * ACQ_FREQ - 50,  xmin * ACQ_FREQ + 50)\n",
    "            data_range = raw_data.loc[ixmin:ixmax ,labels[j]]\n",
    "            x_range = raw_data.loc[ixmin:ixmax, \"Abs Time\"]\n",
    "            axs[j][i].set_xlim(xmax, xmin)\n",
    "\n",
    "        axs[j][i].plot(x_range, data_range, label=labels[j]) #c='b' ?\n",
    "        axs[j][i].set_xlabel(\"Time [s]\")\n",
    "\n",
    "        if \"Acc\" in labels[j]:\n",
    "            axs[j][i].set_ylabel(\"Linear Acceleration [mg]\")\n",
    "        elif \"Gyro\" in labels[j]:\n",
    "            axs[j][i].set_ylabel(\"Angular Velocity [dps]\")\n",
    "        elif \"Magn\" in labels[j]:\n",
    "            axs[j][i].set_ylabel(\"Magnetic Field [mgauss]\")\n",
    "\n",
    "        # axs[j][i].legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "Now we print for all the columns remained the mean, standard deviation, minimum, maximum and the 25,50,75 percentiles in the time window we have selected. Then we also normalize the data by subtracting the mean and by dividing for the standard deviation. For the accelerations and the angular velocities the 25 and 75 percentiles are nearly simmetric, so the variables change quite regularly. We notice a higher deviation for the acceleration over X, and the angular velocity along Z. We plot also the normalized data in a 9x9 grid in order to notice some correlation but in our case nothing interesting appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_cut.drop([\"Log Freq\", \"Timestamp\", \"Abs Time\"], axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = (raw_data_cut[labels] - np.mean(raw_data_cut[labels], axis=0)) / np.std(raw_data_cut[labels], axis=0)\n",
    "data_std.insert(0, \"Abs Time\", raw_data_cut[\"Abs Time\"])\n",
    "data_std.set_index(np.arange(0, len(data_std)), inplace=True)\n",
    "display(data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Now we're compute a PCA (principal component analysis) on the entire dataset (9 columns of data). We select the six components that allow us to maintain the 85% of the variability of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avls, avts = linalg.eig(np.cov(data_std[labels].T))\n",
    "sort_perm = np.flip(np.argsort(avls))\n",
    "\n",
    "avls = np.real_if_close(avls[sort_perm])\n",
    "avts = avts[:, sort_perm]\n",
    "\n",
    "var_ratios = avls / np.sum(avls)\n",
    "print('Eigenvalues:\\n', np.round(avls, 4)) #todo tenere la troncatura?\n",
    "print('Variability ratios:\\n', np.round(var_ratios * 100, 2))\n",
    "print(\"Eigenvector:\")\n",
    "display(pd.DataFrame(avts, columns=[f\"Avt {i}\" for i in range(len(avls))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_LABELS = [f\"PC{i+1}\" for i in range(len(avls))]\n",
    "data_rot = pd.DataFrame(data=np.dot(avts.T, data_std[labels].T).T, columns=PCA_LABELS)\n",
    "\n",
    "perc_soil = .85\n",
    "N_PCA = np.argmax(np.cumsum(var_ratios) >= perc_soil) + 1\n",
    "\n",
    "print(f'To keep {int(perc_soil * 100)}% of our data we need {N_PCA} of the {len(var_ratios)} principal components')\n",
    "\n",
    "display(data_rot)\n",
    "\n",
    "PCA_LABELS = PCA_LABELS[:N_PCA]\n",
    "data_pca = data_rot[data_rot.columns[:N_PCA]]\n",
    "data_pca.insert(0, \"Abs Time\", data_std[\"Abs Time\"])\n",
    "display(data_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter\n",
    "\n",
    "Now we're going to perform a fourier analysis using the scipy fftpack library; in this way we would like to estrapolate the principal frequencies of the data and see if we can see a frequence near the respiratory rate (and also the hearth rate frequency). Firstly we calculate the power and the frequencies and we plot the power w.r.t. the frequency for every variable. In the left column we can see the entire spectrum (with both the respiration's and the hearth's frequencies) while in the right column there is a zoom in the [0,1] frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_fft = fft.fftshift(fft.fft(data_pca[PCA_LABELS], axis=0))\n",
    "sample_freq = fft.fftshift(fft.fftfreq(len(data_pca), d=1/ACQ_FREQ))\n",
    "\n",
    "plt.figure(figsize=(18, 5 * N_PCA))\n",
    "for i in range(N_PCA):\n",
    "    for j in range(2):\n",
    "        ax = plt.subplot(N_PCA, 2, i * 2 + j + 1)\n",
    "        plt.plot(sample_freq, np.abs(sig_fft[:, i]), label=PCA_LABELS[i])\n",
    "        if j == 0:\n",
    "            plt.xlim([0, 3])\n",
    "        else:\n",
    "            plt.xlim([0, 1])\n",
    "            plt.xticks(np.arange(0, 1, 0.1))\n",
    "        \n",
    "        plt.xlabel(\"Freq [Hz]\")\n",
    "        plt.ylabel(\"Power\")\n",
    "        plt.legend(loc=\"best\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_FFT = np.sum(np.abs(sig_fft), axis=1)\n",
    "plt.plot(sample_freq, total_FFT)\n",
    "plt.xlabel(\"Frequency [Hz]\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.xlim([0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a wavelet transform analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pywt.dwt_max_level(len(data_pca), \"sym5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter signal using wavelets\n",
    "#We used db2, with a high decomposition level \n",
    "#as we are interested only in low frequencies but we want to exclude those near zero\n",
    "#we will thus use the last three detail coefficients to\n",
    "#cover ranges [0.098, 0.78125] Hz \n",
    "\n",
    "lvl = 0\n",
    "if file_name == \"center_sternum.txt\":\n",
    "    lvl = 10\n",
    "else:\n",
    "    lvl = 9\n",
    "\n",
    "coeffs = pywt.wavedec(data_pca[PCA_LABELS], \"sym5\", level=lvl, axis=0)\n",
    "coeffs[0] = np.zeros_like(coeffs[0])\n",
    "for i in range(3, lvl + 1):\n",
    "    coeffs[i] = np.zeros_like(coeffs[i])\n",
    "\n",
    "A10 = pywt.waverec(coeffs, \"sym5\", axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot only approximation wavelet\n",
    "fig, axs = plt.subplots(nrows=N_PCA, ncols=2, figsize=(18, N_PCA*5))\n",
    "\n",
    "for i in range(N_PCA):\n",
    "    for j in range(2):\n",
    "        #axs[i][j].plot(data_std[\"Abs Time\"], data_std[labels[i]], label=(\"Original \"+labels[i]))\n",
    "        #axs[i][j].plot(data_std[\"Abs Time\"], filtered[i], label=(\"Butterworth \"+labels[i]))\n",
    "        #axs[i][j].plot(data_std[\"Abs Time\"], A7[i][:-1],  label=(\"Butterworth+Wavelet \"+labels[i]))\n",
    "        axs[i][j].plot(data_pca[\"Abs Time\"], A10[:-1, i], label=(\"Wavelet \"+PCA_LABELS[i]))\n",
    "        if j == 1:\n",
    "            axs[i][j].set_xlim([20,24])\n",
    "        \n",
    "        axs[i][j].set_xlabel(\"Time [s]\")\n",
    "        axs[i][j].set_ylabel(\"Magnitude\") # ???\n",
    "        axs[i][j].legend(loc=\"best\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_fft_wt = fft.fftshift(fft.fft(A10[:-1, :], axis=0))\n",
    "sample_freq = fft.fftshift(fft.fftfreq(len(data_pca), d=1/ACQ_FREQ))\n",
    "\n",
    "plt.figure(figsize=(18, 5 * N_PCA))\n",
    "for i in range(N_PCA):\n",
    "    for j in range(2):\n",
    "        ax = plt.subplot(N_PCA, 2, i * 2 + j + 1)\n",
    "        plt.plot(sample_freq, np.abs(sig_fft[:, i]), label=PCA_LABELS[i])\n",
    "        plt.plot(sample_freq, np.abs(sig_fft_wt[:, i]),\n",
    "                 label=\"Approximated \" + PCA_LABELS[i])\n",
    "        if j == 0:\n",
    "            plt.xlim([0, 3])\n",
    "        else:\n",
    "            plt.xlim([0, 1])\n",
    "            plt.xticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "        plt.xlabel(\"Freq [Hz]\")\n",
    "        plt.ylabel(\"Power\")\n",
    "        plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection of the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A10=A10.T\n",
    "peaks = []\n",
    "valleys = []\n",
    "for i in range(len(A10)):\n",
    "    temp_signal = A10[i][:-1]\n",
    "    temp_peaks, _ = signal.find_peaks(temp_signal)\n",
    "    temp_valleys, _ = signal.find_peaks(-temp_signal)\n",
    "    peaks.append(temp_signal[temp_peaks])\n",
    "    peaks.append(data_std['Abs Time'].values[temp_peaks])\n",
    "    valleys.append(temp_signal[temp_valleys])\n",
    "    valleys.append(data_std['Abs Time'].values[temp_valleys])\n",
    "    \n",
    "fig, ax = plt.subplots(len(A10), 1, figsize=(20, 15))\n",
    "for i in range(1, len(peaks), 2):    \n",
    "    ax[int(i/2)].plot(data_std['Abs Time'], A10[int(i/2)][:-1])\n",
    "    ax[int(i/2)].plot(peaks[i], peaks[i-1], 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_peak_dist = []\n",
    "for i in range(1, len(peaks), 2):    \n",
    "    avg_peak_dist.append(len(data_std['Abs Time']) / len(peaks[i]))\n",
    "print(avg_peak_dist)\n",
    "\n",
    "sign_amp = []\n",
    "for i in range(1, len(peaks), 2):\n",
    "    temp_sign_amp = []    \n",
    "    for j in range(np.min([len(peaks[i]), len(valleys[i])])):\n",
    "        temp_peaks = np.array(peaks[i-1])\n",
    "        temp_valleys = np.array(valleys[i-1])\n",
    "        temp_sign_amp.append(0.5*np.abs(temp_peaks[j] + temp_valleys[j]))\n",
    "    sign_amp.append(temp_sign_amp)\n",
    "\n",
    "avg_sign_amp = np.empty(shape=(len(A10)))\n",
    "for i,x  in enumerate(sign_amp):\n",
    "    avg_sign_amp[i] = np.mean(x)\n",
    "\n",
    "height_perc = .6\n",
    "distance_perc = .7\n",
    "peaks_refined = []\n",
    "valleys_refined = []\n",
    "for i in range(len(A10)):\n",
    "    temp_signal = A10[i][:-1]\n",
    "    temp_peaks, _ = signal.find_peaks(temp_signal, height = height_perc * avg_sign_amp[i], distance = distance_perc * avg_peak_dist[i])\n",
    "    temp_valleys, _ = signal.find_peaks(-temp_signal)\n",
    "    peaks_refined.append(temp_signal[temp_peaks])\n",
    "    peaks_refined.append(data_std['Abs Time'].values[temp_peaks])\n",
    "    valleys_refined.append(temp_signal[temp_valleys])\n",
    "    valleys_refined.append(data_std['Abs Time'].values[temp_valleys])\n",
    "\n",
    "fig, ax = plt.subplots(len(A10), 1, figsize=(20, 15))\n",
    "for i in range(1, len(peaks_refined), 2):    \n",
    "    ax[int(i/2)].plot(data_std['Abs Time'], A10[int(i/2)][:-1])\n",
    "    ax[int(i/2)].plot(peaks_refined[i], peaks_refined[i-1], 'x')\n",
    "    ax[int(i/2)].axhline(avg_sign_amp[int(i/2)] * height_perc, color='g')\n",
    "    # ax[int(i/2)].plot(valleys_refined[i], valleys_refined[i-1], 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate histogram and perform gaussian fit to estimate RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dist = []\n",
    "for i in range(1, len(peaks_refined), 2):\n",
    "    temp_time_dist = []    \n",
    "    for j in range(1, len(peaks_refined[i])):\n",
    "        temp_peaks_time = peaks_refined[i]\n",
    "        temp_time_dist.append(temp_peaks_time[j]-temp_peaks_time[j-1])\n",
    "    time_dist.append(temp_time_dist)\n",
    "\n",
    "time_dist = np.concatenate(time_dist)\n",
    "print(len(time_dist))\n",
    "original_mean = np.mean(time_dist)\n",
    "print(original_mean)\n",
    "original_std = np.std(time_dist)\n",
    "print(original_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bins = ax.hist(x=time_dist)\n",
    "lower_sigma_tol = 2\n",
    "upper_sigma_tol = 3\n",
    "# ax.axvline(np.mean(time_dist) - lower_sigma_tol*original_std, color='r')\n",
    "# ax.axvline(np.mean(time_dist) + upper_sigma_tol*original_std, color='r')\n",
    "\n",
    "bin_centers = (bins[1][:-1] + bins[1][1:]) / 2\n",
    "bin_counts = bins[0]\n",
    "\n",
    "def myround(x):\n",
    "    return round(x,1)\n",
    "\n",
    "ax.set_xticks(bin_centers)\n",
    "_=ax.set_xticklabels(map(myround,bin_centers))\n",
    "\n",
    "def my_gaus(x, A, mu, sigma):\n",
    "    return A * np.exp(-0.5 * ((x-mu)/sigma) ** 2)\n",
    "\n",
    "params, params_cov = optimize.curve_fit(my_gaus, bin_centers, bin_counts, p0=[1, np.mean(time_dist), np.std(time_dist)], absolute_sigma=True, bounds=(0,[100, 100, 100]))\n",
    "fit_domain = np.sort(np.random.uniform(np.min(bin_centers), np.max(bin_centers), 1000))\n",
    "_=ax.plot(fit_domain, my_gaus(fit_domain, *params))\n",
    "\n",
    "print('Fit parameters:\\n',params)\n",
    "print('Fit parameters errors:\\n', np.sqrt(np.diag(params_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative method without filter**\n",
    "\n",
    "When we have no filter we can consider the frequencies' range [0.1,0.4] and compute the respiration rate as the mean of the frequencies of the three max power in the fourier analysis for each of the six principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_freqi = (sample_freq > 0.1) & (sample_freq < 0.4)\n",
    "power_fft = np.abs(sig_fft)\n",
    "\n",
    "freqi_maxs = np.argmax(power_fft[sample_freqi, :], axis=0)\n",
    "freq_maxs = sample_freq[sample_freqi][freqi_maxs]\n",
    "RR = np.mean(freq_maxs)\n",
    "\n",
    "print(f\"RPM is: {RR * 60:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter signal with Butterworth bandpass filter in [0.1,0.5] Hz range\n",
    "\n",
    "freq=200\n",
    "filtered = np.zeros((Nvar,len(data_std)))\n",
    "sos = signal.butter(4, [0.1, 0.8], 'bandpass', fs=freq, output='sos')\n",
    "for i in range(Nvar):\n",
    "    filtered[i] = signal.sosfilt(sos, data_std[labels[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot filtered Gyro and Acc\n",
    "fig, axs = plt.subplots(nrows=Nvar, ncols=2, figsize=(18,Nvar*5))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(Nvar):\n",
    "        axs[j][i].plot(data_std[\"Abs Time\"], data_std[labels[j]], label=labels[j])\n",
    "        axs[j][i].plot(data_std[\"Abs Time\"], filtered[j],         label=(\"Filtered \"+labels[j]))\n",
    "        if i == 1:\n",
    "            axs[j][i].set_xlim([20,24])\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"Magnitude\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate spectrum after filtering\n",
    "sig_fft_filt = 1j*np.zeros((Nvar, len(data_std)))\n",
    "\n",
    "for i in range(Nvar):\n",
    "    sig_fft_filt[i] = fftpack.fft(filtered[i])\n",
    "\n",
    "power_filt = np.abs(sig_fft_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot filtered spectrum\n",
    "fig, axs = plt.subplots(nrows=Nvar, ncols=2, figsize=(18,Nvar*5))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(Nvar):\n",
    "        axs[j][i].plot(sample_freq, power[j],      label=labels[j])\n",
    "        axs[j][i].plot(sample_freq, power_filt[j], label=(\"Filtered \"+labels[j]))\n",
    "        if i == 0:\n",
    "            axs[j][0].set_xlim([0,3])\n",
    "        else:\n",
    "            axs[j][i].set_xlim([0,1])\n",
    "            axs[j][i].set_xticks(np.arange(0,1,0.1))\n",
    "        \n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Freq [Hz]\")\n",
    "    ax.set_ylabel(\"Power\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter signal using wavelets\n",
    "#We used db6 as it is smooth, with a decomposition level=7 \n",
    "#as we are interested only in low frequencies, and the approximate coefficients at level J\n",
    "#cover ranges [0, freq/2^(J+1)], so or the last decomposition [0, 0.78125] Hz\n",
    "lvl = 0\n",
    "if file_name == \"center_sternum.txt\":\n",
    "    lvl = 7\n",
    "else:\n",
    "    lvl = 6\n",
    "    \n",
    "A7 = np.zeros((Nvar, len(data_std)+1))\n",
    "for i in range(Nvar):\n",
    "    coeffs = pywt.wavedec(filtered[i], \"db6\", level=lvl)\n",
    "    for l in range(1, lvl+1):\n",
    "        coeffs[l] = np.zeros_like(coeffs[l])\n",
    "    A7[i] = pywt.waverec(coeffs, \"db6\") \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot only approximation wavelet\n",
    "fig, axs = plt.subplots(nrows=Nvar, ncols=2, figsize=(18,Nvar*5))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(Nvar):\n",
    "        axs[j][i].plot(data_std[\"Abs Time\"], A7[j][:-1], label=labels[j])\n",
    "        if i == 1:\n",
    "            axs[j][i].set_xlim([20,24])\n",
    "        \n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"Magnitude\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate spectrum after wavelet\n",
    "sig_fft_wt = 1j*np.zeros((Nvar, len(data_std)))\n",
    "\n",
    "for i in range(Nvar):\n",
    "    sig_fft_wt[i] = fftpack.fft(A7[i][:-1])\n",
    "\n",
    "power_wt = np.abs(sig_fft_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot filtered spectrum\n",
    "fig, axs = plt.subplots(nrows=Nvar, ncols=2, figsize=(18,Nvar*5))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(Nvar):\n",
    "        axs[j][i].plot(sample_freq, power[j],    label=labels[j])\n",
    "        axs[j][i].plot(sample_freq, power_wt[j], label=(\"Approximated \"+labels[j]))\n",
    "        if i == 0:\n",
    "            axs[j][0].set_xlim([0,3])\n",
    "        else:\n",
    "            axs[j][i].set_xlim([0,1])\n",
    "            axs[j][i].set_xticks(np.arange(0,1,0.1))\n",
    "        \n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlabel(\"Freq [Hz]\")\n",
    "    ax.set_ylabel(\"Power\")\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "facef925bc4600fb61034d39cee95b57bae21946036c03f370e6273d3ef2b545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
